{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6261877,"sourceType":"datasetVersion","datasetId":3599110}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/moazeldsokyx/dogs-vs-cats-classification-with-xception?scriptVersionId=153090481\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Extracting and Preparing the Dataset\n\n### Explanation:\nIn this notebook cell, we import the necessary module, `zipfile`, to work with zip files. The dataset for our image classification project, \"Dogs vs Cats,\" is stored in a zip file located at the path specified by `zip_file_path`.\n\nWe use the `zipfile.ZipFile()` function to open the zip file in read mode (`'r'`). Then, we loop through each file in the zip archive using the `zip_ref.infolist()` method. Within the loop, we extract each file one by one without saving them to a specific path, which allows us to access the dataset's contents directly from the zip file.\n\nAfter successfully extracting all files from the zip archive, we print a \"Successfully extracted files\" message to indicate that the dataset is ready for further processing and analysis in our image classification project.","metadata":{}},{"cell_type":"code","source":"# # Import the required module to work with zip files\n# import zipfile\n\n# # Set the path to the zip file containing the dataset\n# zip_file_path = r'E:\\Computer vision\\Dogs vs Cats\\dataset.zip'\n\n# # Open the zip file in read mode\n# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n#     # Extract all files one by one without saving them in a specific path\n#     for file_info in zip_ref.infolist():\n#         zip_ref.extract(file_info)\n\n# # Print a success message after all files are extracted\n# print(\"Successfully extracted files.\")\n","metadata":{"id":"PLy3pthUS0D2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Structure Overview\n\n### Explanation:\nIn this notebook cell, we explore the structure of the \"Dogs vs Cats\" dataset, and it appears as follows:\n\n1. **Contents of the Base Directory:**\n   The top-level directory, `base_dir`, contains three subdirectories: `validation`, `test`, and `train`. These subdirectories are crucial for organizing and dividing the dataset into respective sets for training, testing, and validation.\n\n2. **Contents of the Train Directory:**\n   The `train` directory contains two subdirectories: `dogs` and `cats`. These subdirectories likely contain the training images of dogs and cats, respectively. Each image category is placed in a separate subdirectory, facilitating the image classification model's training process.\n\n3. **Contents of the Test Directory:**\n   The `test` directory also contains two subdirectories: `dogs` and `cats`. These subdirectories presumably hold the test images of dogs and cats, respectively. The model will be evaluated on these images to assess its performance on unseen data.\n\n4. **Contents of the Validation Directory:**\n   Similarly, the `validation` directory consists of two subdirectories: `dogs` and `cats`. It is likely that the validation images of dogs and cats are placed in these subdirectories. Validation is essential for tuning hyperparameters and ensuring the model generalizes well on new data.\n\nBy organizing the dataset into these directories, it becomes easier to load, preprocess, and feed the images into the image classification model during the various stages of the project.","metadata":{}},{"cell_type":"code","source":"import os\n\nbase_dir = r'/kaggle/input/dogs-vs-cats/dataset'\n\nprint(\"Contents of base directory:\")\nprint(os.listdir(base_dir))\n\nprint(\"\\nContents of train directory:\")\nprint(os.listdir(f'{base_dir}/train'))\n\nprint(\"\\nContents of test directory:\")\nprint(os.listdir(f'{base_dir}/test'))\n\nprint(\"\\nContents of validation directory:\")\nprint(os.listdir(f'{base_dir}/validation'))","metadata":{"id":"Mp39PPeAETY8","execution":{"iopub.status.busy":"2023-08-07T07:29:07.114438Z","iopub.execute_input":"2023-08-07T07:29:07.115229Z","iopub.status.idle":"2023-08-07T07:29:07.129656Z","shell.execute_reply.started":"2023-08-07T07:29:07.115192Z","shell.execute_reply":"2023-08-07T07:29:07.128279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting Up Image Directories","metadata":{}},{"cell_type":"code","source":"train_dir = os.path.join(base_dir, 'train')\ntest_dir = os.path.join(base_dir, 'test')\nvalidation_dir = os.path.join(base_dir, 'validation')\n\n# Directory with training cat/dog pictures\ntrain_cats_dir = os.path.join(train_dir, 'cats')\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\n\ntest_cats_dir = os.path.join(test_dir, 'cats')\ntest_dogs_dir = os.path.join(test_dir, 'dogs')\n\n# Directory with validation cat/dog pictures\nvalidation_cats_dir = os.path.join(validation_dir, 'cats')\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')\n\n","metadata":{"id":"MLZKVtE0dSfk","execution":{"iopub.status.busy":"2023-08-07T10:07:14.582315Z","iopub.execute_input":"2023-08-07T10:07:14.58272Z","iopub.status.idle":"2023-08-07T10:07:14.590858Z","shell.execute_reply.started":"2023-08-07T10:07:14.582691Z","shell.execute_reply":"2023-08-07T10:07:14.589688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cat_fnames = os.listdir( train_cats_dir )\ntrain_dog_fnames = os.listdir( train_dogs_dir )\n\nprint(train_cat_fnames[:10])\nprint(train_dog_fnames[:10])","metadata":{"id":"4PIP1rkmeAYS","execution":{"iopub.status.busy":"2023-08-07T10:07:17.763291Z","iopub.execute_input":"2023-08-07T10:07:17.763691Z","iopub.status.idle":"2023-08-07T10:07:17.782403Z","shell.execute_reply.started":"2023-08-07T10:07:17.763661Z","shell.execute_reply":"2023-08-07T10:07:17.78152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's find out the total number of cat and dog images in the `train` and `validation` directories:","metadata":{"id":"HlqN5KbafhLI"}},{"cell_type":"markdown","source":"## Distribution of Cat and Dog Images\n\n### Explanation:\nIn this notebook cell, we visualize the distribution of cat and dog images across the different data splits, namely training, validation, and test sets. We utilize the Plotly library to create a bar plot that provides a visual representation of the image counts.\n\n1. **Counting Images:**\n   We use the `os.listdir()` function combined with the `len()` function to count the number of images in each category (cats and dogs) for the training, validation, and test sets. The variables `train_cat_count`, `train_dog_count`, `validation_cat_count`, `validation_dog_count`, `test_cat_count`, and `test_dog_count` store these counts for their respective directories.\n\n2. **Bar Plot:**\n   We create a bar plot using Plotly's `go.Bar()` function. The x-axis represents the data split categories ('Training', 'Validation', and 'Test'), and the y-axis displays the number of images for each category. We create two bars for each data split: one for cats and another for dogs. The height of each bar corresponds to the number of images in the corresponding category.\n\n3. **Layout and Labels:**\n   We update the layout of the plot using the `update_layout()` function to add a title and axis labels. The title of the plot is set to 'Distribution of Cat and Dog Images'. The x-axis label is 'Data Split', indicating the different sets, and the y-axis label is 'Number of Images', representing the count of images.\n\nBy visualizing the distribution of cat and dog images across the different data splits, we can gain insights into the balance of the dataset and ensure that each category has a sufficient number of images for effective model training and evaluation.","metadata":{}},{"cell_type":"code","source":"import plotly.graph_objects as go\n\n\n# Count the number of images in each directory\ntrain_cat_count = len(os.listdir(train_cats_dir))\ntrain_dog_count = len(os.listdir(train_dogs_dir ))\nvalidation_cat_count = len(os.listdir(validation_cats_dir))\nvalidation_dog_count = len(os.listdir(validation_dogs_dir))\ntest_cat_count = len(os.listdir(test_cats_dir))\ntest_dog_count = len(os.listdir(test_dogs_dir))\n\n# Create a bar plot using Plotly\ncategories = ['Training', 'Validation', 'Test']\ncats_counts = [train_cat_count, validation_cat_count, test_cat_count]\ndogs_counts = [train_dog_count, validation_dog_count, test_dog_count]\n\nfig = go.Figure(data=[\n    go.Bar(name='Cats', x=categories, y=cats_counts),\n    go.Bar(name='Dogs', x=categories, y=dogs_counts)\n])\n\n# Update the layout to add titles and labels\nfig.update_layout(title='Distribution of Cat and Dog Images',\n                  xaxis_title='Data Split',\n                  yaxis_title='Number of Images')\n\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-07T10:07:22.273748Z","iopub.execute_input":"2023-08-07T10:07:22.274206Z","iopub.status.idle":"2023-08-07T10:07:22.694684Z","shell.execute_reply.started":"2023-08-07T10:07:22.274148Z","shell.execute_reply":"2023-08-07T10:07:22.69355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('total training cat images :', train_cat_count)\nprint('total training dog images :', train_dog_count)\n\nprint('total validation cat images :', validation_cat_count)\nprint('total validation dog images :', validation_dog_count)\n\nprint('total test cat images :', test_cat_count)\nprint('total test dog images :', test_dog_count)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T10:09:37.892855Z","iopub.execute_input":"2023-08-07T10:09:37.893318Z","iopub.status.idle":"2023-08-07T10:09:37.900721Z","shell.execute_reply.started":"2023-08-07T10:09:37.893286Z","shell.execute_reply":"2023-08-07T10:09:37.899515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Displaying a Subset of Cat and Dog Images","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\n\n# Parameters for our graph; we'll output images in a 4x4 configuration\nnrows = 4\nncols = 4\n\npic_index = 0 # Index for iterating over images","metadata":{"id":"b2_Q0-_5UAv-","execution":{"iopub.status.busy":"2023-08-07T07:33:30.32658Z","iopub.execute_input":"2023-08-07T07:33:30.327021Z","iopub.status.idle":"2023-08-07T07:33:30.33471Z","shell.execute_reply.started":"2023-08-07T07:33:30.32699Z","shell.execute_reply":"2023-08-07T07:33:30.333768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up matplotlib fig, and size it to fit 4x4 pics\nfig = plt.gcf()\nfig.set_size_inches(ncols*4, nrows*4)\n\npic_index+=8\n\nnext_cat_pix = [os.path.join(train_cats_dir, fname) \n                for fname in train_cat_fnames[ pic_index-8:pic_index] \n               ]\n\nnext_dog_pix = [os.path.join(train_dogs_dir, fname) \n                for fname in train_dog_fnames[ pic_index-8:pic_index]\n               ]\n\nfor i, img_path in enumerate(next_cat_pix+next_dog_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows, ncols, i + 1)\n  sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()\n","metadata":{"id":"Wpr8GxjOU8in","execution":{"iopub.status.busy":"2023-08-07T07:33:33.868735Z","iopub.execute_input":"2023-08-07T07:33:33.869223Z","iopub.status.idle":"2023-08-07T07:33:35.68705Z","shell.execute_reply.started":"2023-08-07T07:33:33.869161Z","shell.execute_reply":"2023-08-07T07:33:35.685548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Pre-trained Xception Model\n\n### Explanation:\nIn this notebook cell, we use TensorFlow and Keras to load a pre-trained Xception model for transfer learning in our \"Dogs vs Cats\" image classification project.\n\n1. **Importing Required Modules:**\n   We import the necessary modules from TensorFlow and Keras to work with the Xception model and image data.\n\n2. **Xception Model Loading:**\n   We load the pre-trained Xception model using the `Xception` class from `tensorflow.keras.applications.xception`. The model is initialized with the weights set to 'imagenet', indicating that we want to use the weights pre-trained on the ImageNet dataset. We also set `include_top=False` to exclude the top classification layer of the Xception model. By excluding the top layer, we can add our own classification layer tailored to the \"Dogs vs Cats\" image classification task.\n\n3. **Input Shape:**\n   The `input_shape` parameter is set to (299, 299, 3), representing the input shape of the images that will be fed into the Xception model. The Xception model requires images with a size of 299x299 pixels and three color channels (RGB).\n\nBy loading the pre-trained Xception model, we can leverage the model's feature extraction capabilities and then fine-tune it with our custom classification layer to achieve high accuracy in the \"Dogs vs Cats\" image classification task.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.xception import Xception, preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers, models, optimizers\n\n# Load the pre-trained Xception model without the top classification layer\nbase_model = Xception(weights='imagenet', include_top=False, input_shape=(299, 299, 3))","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:36:16.696166Z","iopub.execute_input":"2023-08-07T07:36:16.696599Z","iopub.status.idle":"2023-08-07T07:36:18.431933Z","shell.execute_reply.started":"2023-08-07T07:36:16.696568Z","shell.execute_reply":"2023-08-07T07:36:18.430839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building the Transfer Learning Model\n\n### Explanation:\nIn this notebook cell, we build a transfer learning model for the \"Dogs vs Cats\" image classification task by combining a pre-trained Xception base model with a custom classification head.\n\n1. **Freezing Base Model Layers:**\n   We set `base_model.trainable = False` to freeze the layers of the pre-trained Xception model. By freezing the layers, we prevent them from being trained during the fine-tuning process. This is beneficial because the pre-trained weights of the Xception model already capture useful features that can be utilized for our classification task, and we do not want to distort them by further training.\n\n2. **Creating the Custom Classification Head:**\n   We create a new Keras Sequential model (`model`) by stacking layers on top of the frozen Xception base model. The custom classification head includes the following layers:\n   - `base_model`: This is the frozen pre-trained Xception base model that acts as a feature extractor.\n   - `GlobalAveragePooling2D()`: This layer performs global average pooling, reducing the spatial dimensions of the extracted features.\n   - `Dense(256, activation='relu')`: A fully connected (dense) layer with 256 units and ReLU activation. This layer helps in capturing higher-level representations from the global average pooled features.\n   - `Dropout(0.5)`: A dropout layer with a rate of 0.5, which helps in regularizing the model and preventing overfitting during training.\n   - `Dense(1, activation='sigmoid')`: The final dense layer with a single unit and a sigmoid activation function. This layer outputs a probability score, indicating whether the input image is a cat or a dog (binary classification).\n\nBy combining the pre-trained Xception base model with the custom classification head, we create an effective transfer learning model for the \"Dogs vs Cats\" image classification task. The pre-trained Xception model brings valuable learned features, and the custom classification head tailors the model to our specific classification problem.","metadata":{}},{"cell_type":"code","source":"# Freeze the base model's layers so they are not trained during fine-tuning\nbase_model.trainable = False\n\n# Add a new classification head for dogs and cats classification\nmodel = models.Sequential([\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:36:22.817013Z","iopub.execute_input":"2023-08-07T07:36:22.817525Z","iopub.status.idle":"2023-08-07T07:36:23.387957Z","shell.execute_reply.started":"2023-08-07T07:36:22.817491Z","shell.execute_reply":"2023-08-07T07:36:23.386572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Generators for Training and Validation\n\n### Explanation:\nIn this notebook cell, we set up data generators for the training and validation sets of the \"Dogs vs Cats\" image classification task. Data generators are useful for efficiently loading and augmenting images in batches during model training, which helps prevent memory overflow and allows for real-time data augmentation.\n\n1. **Data Directories:**\n   We specify the paths to the training and validation data directories using `train_data_dir` and `validation_data_dir`, respectively. These directories contain the organized dataset images for their respective sets.\n\n2. **Batch Size and Image Size:**\n   We set the `batch_size` to 32, which means the data generator will load and process 32 images at a time during training and validation. The `image_size` is set to (299, 299) to match the input size required by the Xception model.\n\n3. **Training Data Generator:**\n   We create an `ImageDataGenerator` object for the training set using `train_datagen`. This generator performs data augmentation on the training images, including shear, zoom, and horizontal flip, to increase the diversity of the data. The `preprocess_input` function is applied to each image, ensuring that the input is preprocessed according to the requirements of the Xception model.\n\n4. **Train Generator Flow:**\n   We use the `flow_from_directory` method of the training data generator to create a `train_generator`. This generator will yield batches of training data, where each batch contains images and their corresponding labels (binary values: 0 for cats, 1 for dogs).\n\n5. **Validation Data Generator:**\n   Similarly, we create an `ImageDataGenerator` object for the validation set using `validation_datagen`. This generator is not configured for data augmentation and only applies the `preprocess_input` function for preprocessing.\n\n6. **Validation Generator Flow:**\n   We use the `flow_from_directory` method of the validation data generator to create a `validation_generator`. Like the training generator, this generator will yield batches of validation data with their respective labels.\n\nBy using data generators, we ensure that the model efficiently accesses and processes the data during training and validation, leading to better utilization of system resources and faster convergence during training. Additionally, data augmentation in the training generator helps improve the model's generalization ability by exposing it to a diverse range of image variations.","metadata":{}},{"cell_type":"code","source":"# Set up data generators for training and validation sets\ntrain_data_dir = r'/kaggle/input/dogs-vs-cats/dataset/train'\nvalidation_data_dir = r'/kaggle/input/dogs-vs-cats/dataset/validation'\n\nbatch_size = 32\nimage_size = (299, 299)\n\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\nvalidation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\nvalidation_generator = validation_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='binary'\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:37:30.028227Z","iopub.execute_input":"2023-08-07T07:37:30.028654Z","iopub.status.idle":"2023-08-07T07:37:40.484498Z","shell.execute_reply.started":"2023-08-07T07:37:30.028621Z","shell.execute_reply":"2023-08-07T07:37:40.483436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transfer Learning with Large Dataset and Limited Epochs\n\n### Explanation:\nIn this notebook cell, we employ transfer learning with a large dataset of \"Dogs vs Cats\" images, and we train the model for only one epoch.\n\n1. **Transfer Learning:**\n   Transfer learning is used in this scenario to leverage the pre-trained Xception model's learned features on the ImageNet dataset. By setting `base_model.trainable = False`, we freeze the base model's layers to prevent further training of these weights during fine-tuning. This approach is particularly useful when working with a limited amount of labeled data for a specific task.\n\n2. **Large Dataset:**\n   Our dataset contains a substantial number of images, which could take a considerable amount of time to process and train. Using data generators helps mitigate memory limitations, enabling us to train efficiently with large datasets without loading all images into memory simultaneously.\n\n3. **Limited Epochs:**\n   Due to the time and computational resources required for training on a large dataset, we set `epochs` to 1 for demonstration purposes. This means we perform only one pass through the entire training dataset during training. In practice, you may need to train the model for more epochs to achieve better convergence and improve accuracy.\n\n4. **High Accuracy with One Epoch:**\n   While one epoch is insufficient for optimal convergence and high accuracy, transfer learning allows the model to start with pre-learned features, giving it an advantage over training from scratch. Therefore, even with just one epoch, the model may achieve reasonable accuracy levels. However, it is essential to note that a single epoch is not sufficient for capturing the full potential of the model and may not generalize well to unseen data.\n\n5. **Fine-Tuning:**\n   For further improvements in accuracy, consider fine-tuning the model by unfreezing some of the top layers of the base model. Fine-tuning allows the model to adapt to the specific characteristics of the \"Dogs vs Cats\" classification task. You can continue the training process with a smaller learning rate to fine-tune the model's weights to the target domain while preserving the pre-trained knowledge.\n\nIn summary, while transfer learning and data generators are valuable tools for efficiently training on large datasets, achieving high accuracy requires tuning hyperparameters, including the number of epochs, and potentially performing fine-tuning to adapt the model to the task at hand.","metadata":{}},{"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer=optimizers.Adam(),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model\nepochs = 1\nsteps_per_epoch = train_generator.n // batch_size\nvalidation_steps = validation_generator.n // batch_size\n\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=validation_steps\n)\nmodel.save(\"model.h5\")\n","metadata":{"execution":{"iopub.status.busy":"2023-08-07T07:38:20.0338Z","iopub.execute_input":"2023-08-07T07:38:20.034765Z","iopub.status.idle":"2023-08-07T09:11:47.334558Z","shell.execute_reply.started":"2023-08-07T07:38:20.034728Z","shell.execute_reply":"2023-08-07T09:11:47.333442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicting Some Random Images\n\nin this notebook cell, we use the trained model to predict the labels of some random images from the test directory. The images are randomly selected from both the dog and cat categories, and their predicted labels (either \"dog\" or \"cat\") are displayed.","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import img_to_array\n\n\ndef predict_image(img):\n    # Preprocess the image if needed\n    x = img_to_array(img)\n    x = cv2.resize(x, (299, 299), interpolation=cv2.INTER_AREA)\n    x /= 255\n    x = np.expand_dims(x, axis=0)\n    image = np.vstack([x])\n    prediction = model.predict(image)\n    predicted_label = \"dog\" if prediction > 0.5 else \"cat\"\n    return predicted_label\n\ndef load_images_from_directory(directory, num_images=10):\n    images = []\n    filenames = random.sample(os.listdir(directory), num_images)\n    for filename in filenames:\n        img_path = os.path.join(directory, filename)\n        img = cv2.imread(img_path)\n        images.append(img)\n    return images\n\ndog_directory = '/kaggle/input/dogs-vs-cats/dataset/test/dogs'\ncat_directory = '/kaggle/input/dogs-vs-cats/dataset/test/cats'\n\ndog_images = load_images_from_directory(dog_directory)\ncat_images = load_images_from_directory(cat_directory)\n\n# Create a 2x10 grid to display the images\nfig, axes = plt.subplots(2, 10, figsize=(15, 5))\nfig.suptitle('Dog vs. Cat Classification', fontsize=16)\n\n# Loop through the dog images\nfor i in range(10):\n    img = dog_images[i]\n    label = predict_image(img)\n    axes[0, i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    axes[0, i].axis('off')\n    axes[0, i].set_title(label)\n\n# Loop through the cat images\nfor i in range(10):\n    img = cat_images[i]\n    label = predict_image(img)\n    axes[1, i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    axes[1, i].axis('off')\n    axes[1, i].set_title(label)\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-08T06:58:58.811636Z","iopub.execute_input":"2023-08-08T06:58:58.812063Z","iopub.status.idle":"2023-08-08T06:59:09.232291Z","shell.execute_reply.started":"2023-08-08T06:58:58.812025Z","shell.execute_reply":"2023-08-08T06:59:09.230924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating Test Set Accuracy \n\n### Explanation:\nIn this notebook cell, we evaluate the test set accuracy for the \"Dogs vs Cats\" image classification model. However, instead of calculating accuracy on the entire test set, we opt to evaluate accuracy on a random subset of 25% of the test images.\n\n1. **Test Data Directory:**\n   We set `test_data_dir` to the path of the folder containing the test images.\n\n2. **Test Data Generator:**\n   We create a data generator for the test set using `test_datagen`. The test data generator preprocesses the images but does not shuffle them, ensuring that images are predicted in the same order as they are loaded.\n\n3. **Batch Size and Image Size:**\n   The `batch_size` is set to 32, and `image_size` is set to (299, 299) to match the input size required by the Xception model.\n\n4. **Test Data Generator Flow:**\n   We use the `flow_from_directory` method of the test data generator to create a `test_generator`. This generator will yield batches of test data with their respective labels (binary values: 0 for cats, 1 for dogs).\n\n5. **Calculating Steps:**\n   To evaluate accuracy on a subset, we calculate the number of steps needed to predict 25% of the test set. We use `total_test_samples` to get the total number of test samples and `predict_percentage` to specify the percentage of the test set to be used for evaluation. We then calculate `steps_per_epoch` based on the batch size.\n\n6. **Predicting Classes:**\n   We use the trained model to predict the classes of the test images. The predictions are stored in the `predictions` array, which contains the probability scores for each image.\n\n7. **Converting to Class Labels:**\n   The predicted probabilities are converted to class labels (1 for dog, 0 for cat) using a threshold of 0.5. The `predicted_labels` array contains the predicted class labels for each image.\n\n8. **True Labels:**\n   We obtain the true class labels from the test generator and store them in the `true_labels` array.\n\n9. **Calculating Accuracy:**\n   The accuracy is calculated by comparing the predicted labels to the true labels. The accuracy is the ratio of correctly predicted samples to the total number of samples.\n\nBy evaluating accuracy on a random subset of 25% of the test images, we can get a quick estimate of the model's performance without waiting for the evaluation on the entire test set. This is especially useful when dealing with large test sets, as calculating accuracy on the full test set could be time-consuming. It's important to note that this subset accuracy provides an approximation of the model's performance, and for a comprehensive evaluation, the entire test set should eventually be used.","metadata":{}},{"cell_type":"code","source":"# Path to the folder containing the test images \ntest_data_dir = r\"/kaggle/input/dogs-vs-cats/dataset/test\"\n\n# Create a data generator for the test set\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\nbatch_size = 32\nimage_size = (299, 299)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_data_dir,\n    target_size=image_size,\n    batch_size=batch_size,\n    class_mode='binary',  # Use 'binary' class_mode for binary classification (cats and dogs)\n    shuffle=False         # Set shuffle to False to ensure images are predicted in the same order as they are loaded\n)\n\n# Calculate the number of steps to predict 25% of the test set\ntotal_test_samples = len(test_generator.filenames)\npredict_percentage = 0.25\nsteps_per_epoch = int(total_test_samples * predict_percentage) // batch_size\n\n# Predict the classes of the test images\npredictions = model.predict(test_generator, steps=steps_per_epoch, verbose=1)\n\n# Convert predicted probabilities to class labels (1: dog, 0: cat)\npredicted_labels = (predictions > 0.5).astype(int)\n\n# Get the true labels from the generator\ntrue_labels = test_generator.classes[:steps_per_epoch * batch_size]\n\n# Calculate accuracy\naccuracy = (predicted_labels == true_labels).mean()\nprint(\"Test accuracy:\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-07T09:37:18.950142Z","iopub.execute_input":"2023-08-07T09:37:18.95066Z","iopub.status.idle":"2023-08-07T09:41:43.322347Z","shell.execute_reply.started":"2023-08-07T09:37:18.950622Z","shell.execute_reply":"2023-08-07T09:41:43.321308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model Deployment**: I successfully deployed my Dogs vs. Cats classification model using Xception on Hugging Face. You can check it out here: https://huggingface.co/spaces/moazx/Dogs-vs-Cats-classification-with-Xception","metadata":{}}]}